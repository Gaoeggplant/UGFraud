{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import csv\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Random_Split(split_per, fake_reviewers, genuine_reviewers):\n",
    "    \n",
    "    minimum = min(split_per * fake_reviewers.shape[0], split_per * genuine_reviewers.shape[0])\n",
    "    \n",
    "    #split for spammers\n",
    "    fake_split_percentage = round(minimum)\n",
    "    fakes_rev = np.random.permutation(fake_reviewers)\n",
    "    labeled_spammers = fakes_rev[0:fake_split_percentage]\n",
    "    \n",
    "    #split for non-spammers\n",
    "    genuine_split_percentage = round(minimum)\n",
    "    genuine_rev = np.random.permutation(genuine_reviewers)\n",
    "    labeled_nonspammers = genuine_rev[0:genuine_split_percentage]\n",
    "    \n",
    "    return (labeled_spammers, labeled_nonspammers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AdjacencyList(metadata_filename):\n",
    "    '''\n",
    "        Extracts adjacency list from metadata file\n",
    "        Args:\n",
    "            metadata_filename: contains [user_id, prod_id, rating, ...]\n",
    "        Returns:\n",
    "            adjacency list [user_id, prod_id, rating]\n",
    "    '''\n",
    "    \n",
    "    fid = open(metadata_filename, 'r')\n",
    "    data = fid.readlines()\n",
    "    adjlist = np.zeros((len(data), 4))\n",
    "    \n",
    "    i = 0\n",
    "    count_fake_review = 0\n",
    "    count_genuine_review = 0\n",
    "    \n",
    "    fake_reviewers = np.zeros(len(data))\n",
    "    genuine_reviewers = np.zeros(len(data))\n",
    "    for edge in data:\n",
    "        items = edge.strip().split()\n",
    "        adjlist[i, 0] = float(items[0])\n",
    "        adjlist[i, 1] = float(items[1])\n",
    "        adjlist[i, 2] = float(items[2])\n",
    "        adjlist[i, 3] = float(items[3])\n",
    "        \n",
    "        if(float(items[3]) == -1):\n",
    "            fake_reviewers[count_fake_review] = adjlist[i, 0]\n",
    "            count_fake_review = count_fake_review + 1\n",
    "            \n",
    "        i = i + 1\n",
    "    #adjlist = np.zeros((14, 3))\n",
    "    #with open(metadata_filename, 'rt') as csvfile:\n",
    "    #    csvreader = csv.reader(csvfile)\n",
    "    #    i = 0\n",
    "    #    for edge in csvreader:\n",
    "    #        adjlist[i, 0] = float(edge[0])\n",
    "    #        adjlist[i, 1] = float(edge[1])\n",
    "    #        adjlist[i, 2] = float(edge[2])\n",
    "    #        i = i + 1\n",
    "\n",
    "    reviewers = np.unique(adjlist[:, 0])\n",
    "    f_reviewers = np.unique(fake_reviewers[0:count_fake_review])\n",
    "    g_reviewers = np.zeros(reviewers.shape[0] - f_reviewers.shape[0])\n",
    "    count = 0\n",
    "    for user in reviewers:\n",
    "        if not user in f_reviewers:\n",
    "            g_reviewers[count] = user\n",
    "            count = count + 1\n",
    "    \n",
    "\n",
    "    return (adjlist, f_reviewers, g_reviewers)\n",
    "\n",
    "#AdjacencyList('metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ReshapePriors(priors):\n",
    "    '''\n",
    "        Reorders ids in ascneding order\n",
    "        Example: ({2: 0.3 0.7, 1: 0.5 0.5, 3: 0.4 0.6}) --> ({1: 0.5 0.5, 2: 0.3 0.7, 3: 0.4 0.6})\n",
    "        Args:\n",
    "            [id, prior_belief, 1-prior_belief]\n",
    "        Returns:\n",
    "            [id, prior_belief, 1-prior_belief] in ascending order of ids\n",
    "    '''\n",
    "    od = collections.OrderedDict(sorted(priors.items()))\n",
    "    \n",
    "    u_ids = np.zeros((len(od), 1))\n",
    "    i=0\n",
    "    for elem in od.keys():\n",
    "        u_ids[i] = elem\n",
    "        i = i + 1\n",
    "    \n",
    "    u_vals = np.zeros((len(od), 1))\n",
    "    i=0\n",
    "    for elem in od.values():\n",
    "        u_vals[i] = elem\n",
    "        i = i + 1   \n",
    "    \n",
    "    u_priors = np.concatenate((u_ids, u_vals), axis = 1)\n",
    "    unew_priors = np.concatenate((u_priors, np.ones((len(od), 1)) - u_vals), axis = 1)\n",
    "    \n",
    "    return unew_priors\n",
    "\n",
    "#print(ReshapePriors({2:3, 1:89, 4:5, 3:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tConstruct account, product and review features given a review dataset.\n",
    "    Author: Sihong Xie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import gzip\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from iohelper import *\n",
    "\n",
    "date_time_format_str = '%Y-%m-%d'\n",
    "\n",
    "\n",
    "\n",
    "def PR_NR(data):\n",
    "\t\"\"\"\n",
    "\t\tRatio of positive and negative reviews of a user or product\n",
    "\t\tArgs:\n",
    "\t\t\tdata is a dictionary with key=u_id or p_id and value = tuples of (neighbor id, rating, label, posting time)\n",
    "\t\tReturn:\n",
    "\t\t\tdictionary with key = u_id or p_id and value = (PR, NR)\n",
    "\t\"\"\"\n",
    "\tfeature = {}\n",
    "\n",
    "\tfor i, d in data.items():\n",
    "\t\tpositives = [1 for t in d if t[1] > 3]\n",
    "\t\tnegatives = [1 for t in d if t[1] < 3]\n",
    "\t\tfeature[i] = (float(len(positives)) / len(d), float(len(negatives)) / len(d))\n",
    "\treturn feature\n",
    "\n",
    "def avgRD_user(user_data, product_data):\n",
    "    \"\"\"\n",
    "        Average rating deviation of each user / product.\n",
    "        For a user i, avgRD(i) = average(r_ij - avg_j | for all r_ij of the user i)\n",
    "        For a product j, avgRD(j) = average(r_ij - avg_j | for all r_ij of the user i) = 0!?\n",
    "        Return:\n",
    "            average rating deviation on users, as defined in the paper\n",
    "            Detecting product review spammers using rating behaviors, CIKM, 2010\n",
    "    \"\"\"\n",
    "    # find the average rating of each product\n",
    "    p_avg = {}\n",
    "    for i, d in product_data.items():\n",
    "        p_avg[i] = np.mean(np.array([t[1] for t in d]))\n",
    "\n",
    "    # find average rating deviation of each user\n",
    "    u_avgRD = {}\n",
    "    for i, d in user_data.items():\n",
    "        u_avgRD[i] = np.mean(np.array([abs(t[1] - p_avg[t[0]]) for t in d]))\n",
    "\n",
    "    return u_avgRD\n",
    "\n",
    "def ERD(data):\n",
    "\t\"\"\"\n",
    "\t\tEntropy of the rating distribution of each user (product)\n",
    "\t\"\"\"\n",
    "\terd = {}\n",
    "\tfor i, d in data.items():\n",
    "\t\tratings = [t[1] for t in d]\n",
    "\t\th, _ = np.histogram(ratings, bins = np.arange(1,7))\n",
    "\t\th = h / h.sum()\n",
    "\t\th = h[np.nonzero(h)]\n",
    "\t\terd[i] = (- h * np.log2(h)).sum()\n",
    "\treturn erd\n",
    "\n",
    "\n",
    "\n",
    "def RD(product_data):\n",
    "\t\"\"\"Calculate the deviation of the review ratings to the product average.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tprod_data:\n",
    "\t\tReturn:\n",
    "\t\t\ta dictionary with key = (u_id, p_id), value = deviation of the rating of this review to the average rating of the target product\n",
    "\t\"\"\"\n",
    "\trd = {}\n",
    "\tfor i, d in product_data.items():\n",
    "\t\tavg = np.mean(np.array([t[1] for t in d]))\n",
    "\t\tfor t in d:\n",
    "\t\t\trd[(t[0], i)] = abs(t[1] - avg)\n",
    "\treturn rd\n",
    "\n",
    "def EXT(product_data):\n",
    "\t\"\"\"\n",
    "\t\tWhether a rating is extreme or not\n",
    "\t\tArgs:\n",
    "\t\t\tproduct_data is a dictionary with key=p_id and value = tuples of (u_id, rating, label, posting time)\n",
    "\t\tReturn:\n",
    "\t\t\ta dictionary with key = (u_id, p_id) and value = 0 (not extreme) / 1 (extreme)\n",
    "\t\"\"\"\n",
    "\text = {}\n",
    "\tfor i, d in product_data.items():\n",
    "\t\tfor t in d:\n",
    "\t\t\tif int(t[1]) == 5 or int(t[1]) == 1:\n",
    "\t\t\t\text[(t[0], i)] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\text[(t[0], i)] = 0\n",
    "\treturn ext\n",
    "\n",
    "def DEV(product_data):\n",
    "\t\"\"\"\n",
    "\t\tDeviation of each rating from the average rating of the target product.\n",
    "\t\tNeed to use \"recursive minimal entropy partitioning\" to find beta_1\n",
    "\t\tArgs:\n",
    "\t\t\tproduct_data is a dictionary with key=p_id and value = tuples of (neighbor id, rating, label, posting time)\n",
    "\t\tReturn:\n",
    "\t\t\ta dictionary with key = (u_id, p_id) and value = (RD_ij, RD_ij / 4 > 0.63 ? 1: 0)\n",
    "\t\t\twhere RD_ij = |r_ij - average rating of product j|\n",
    "\t\"\"\"\n",
    "\tbeta_1 = 0.63\n",
    "\tdev = {}\n",
    "# i is a product id\n",
    "\tfor i, d in product_data.items():\n",
    "\t\t# find the average rating of each product\n",
    "\t\tp_avg_rating = np.mean(np.array([t[1] for t in d]))\n",
    "\t\tfor t in d:\n",
    "\t\t\tu_id = t[0]\t# user id\n",
    "\t\t\tif (abs(p_avg_rating - t[1]) / 4.0 > 0.63):\n",
    "\t\t\t\tdev[(u_id, i)] = 1\t# absolute difference between current rating and product average rating\n",
    "\t\t\telse:\n",
    "\t\t\t\tdev[(u_id, i)] = 0\t# absolute difference between current rating and product average rating\n",
    "\treturn dev\n",
    "\n",
    "\n",
    "\n",
    "def ISR(user_data):\n",
    "\t\"\"\"\n",
    "\t\tCheck if a user posts only one review\n",
    "\t\"\"\"\n",
    "\tisr = {}\n",
    "\tfor i, d in user_data.items():\n",
    "\t\t# go through all review of this user\n",
    "\t\tfor t in d:\n",
    "\t\t\tif len(d) == 1:\n",
    "\t\t\t\tisr[(i, t[0])] = 1\n",
    "\treturn isr\n",
    "\n",
    "\n",
    "\n",
    "def read_graph_data(metadata_filename):\n",
    "    \n",
    "    user_data = {}\n",
    "    \n",
    "    prod_data = {}\n",
    "\n",
    "    # use the rt mode to read ascii strings instead of binary\n",
    "    with open(metadata_filename, 'rt') as f:\n",
    "        # file format: each line is a tuple (user id, product id, rating, label, date)\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            u_id = items[0]\n",
    "            p_id = items[1]\n",
    "            rating = float(items[2])\n",
    "            label = int(items[3])\n",
    "            date = items[4]\n",
    "\n",
    "\n",
    "            if u_id not in user_data:\n",
    "                user_data[u_id] = []\n",
    "            user_data[u_id].append((p_id, rating, label, date))\n",
    "\n",
    "            if p_id not in prod_data:\n",
    "                prod_data[p_id] = []\n",
    "            prod_data[p_id].append((u_id, rating, label, date))\n",
    "\n",
    "\t# read text feature files, including: wordcount, ratio of SW/OW, etc.\n",
    "\t# constructed by the python files provided by the authors.\n",
    "\n",
    "    print ('number of users = %d' % len(user_data))\n",
    "    print ('number of products = %d' % len(prod_data))\n",
    "    return user_data, prod_data\n",
    "\n",
    "def add_feature(existing_features, new_features, feature_names):\n",
    "\t\"\"\"\n",
    "\t\tAdd feature(s) of a set of nodes of the same type to the existing feature(s).\n",
    "\t\tArgs:\n",
    "\t\t\texisting_features: a dictionary {node_id:dict{feature_name:feature_value}}\n",
    "\t\t\tnew_features: new feature(s) to be added. A dict {node_id: list of feature values}\n",
    "\t\t\tfeature_names: the name of the new feature. A list of feature names, in the same order of the list of feature values in new_features\n",
    "\t\"\"\"\n",
    "\n",
    "\tfor k, v in new_features.items():\n",
    "\t\t# k is the node id and v is the feature value\n",
    "\t\tif k not in existing_features:\n",
    "\t\t\texisting_features[k] = dict()\n",
    "\t\t# add the new feature to the dict of the node\n",
    "\t\tfor i in range(len(feature_names)):\n",
    "\t\t\tif len(feature_names) > 1:\n",
    "\t\t\t\texisting_features[k][feature_names[i]] = v[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\texisting_features[k][feature_names[i]] = v\n",
    "\n",
    "#def construct_all_features(metadata_filename, text_feature_filename, user_feature_filename, prod_feature_filename, review_feature_filename):\n",
    "def construct_all_features(user_data, prod_data, text_features):\n",
    "    \"\"\"\n",
    "        Main entry to feature construction.\n",
    "        Args:\n",
    "            metadata_filename:\n",
    "            text_feature_filename:\n",
    "        Return:\n",
    "            user, product and review features\n",
    "    \"\"\"\n",
    "    \n",
    "    # key = user id, value = dict of {feature_name: feature_value}\n",
    "    UserFeatures={}\n",
    "    # key = product id, value = dict of {feature_name: feature_value}\n",
    "    ProdFeatures={}\n",
    "    \n",
    "# go through feature functions\n",
    "    print ('\\nadding user and product features......\\n')\n",
    "\t#uf = MNR(user_data)\n",
    "\t#add_feature(UserFeatures, uf, [\"MNR\"])\n",
    "\t#pf = MNR(prod_data)\n",
    "\t#add_feature(ProdFeatures, pf, [\"MNR\"])\n",
    "    \n",
    "    uf = PR_NR(user_data)\n",
    "    add_feature(UserFeatures, uf, [\"PR\", \"NR\"])\n",
    "    pf = PR_NR(prod_data)\n",
    "    add_feature(ProdFeatures, pf, [\"PR\", \"NR\"])\n",
    "    \n",
    "    uf = avgRD_user(user_data, prod_data)\n",
    "    add_feature(UserFeatures, uf, [\"avgRD\"])\n",
    "\n",
    "\t#uf = BST(user_data)\n",
    "\t#add_feature(UserFeatures, uf, [\"BST\"])\n",
    "\n",
    "    uf = ERD(user_data)\n",
    "    add_feature(UserFeatures, uf, [\"ERD\"])\n",
    "    pf = ERD(prod_data)\n",
    "    add_feature(ProdFeatures, pf, [\"ERD\"])\n",
    "\n",
    "\t#uf = ETG(user_data)\n",
    "\t#add_feature(UserFeatures, uf, [\"ETG\"])\n",
    "\t#pf = ETG(prod_data)\n",
    "\t#add_feature(ProdFeatures, pf, [\"ETG\"])\n",
    "\n",
    "    #MN: Jan 7, 2018 - we don't deal with text-based features\n",
    "\t#uf = RL(user_data, text_features)\n",
    "\t#add_feature(UserFeatures, uf, ['RL'])\n",
    "\t#pf = RL(prod_data, text_features, isUser = False)\n",
    "\t#add_feature(ProdFeatures, pf, ['RL'])\n",
    "\n",
    "# go through review features\n",
    "    print ('\\nadding review features......\\n')\n",
    "    ReviewFeatures = {}\n",
    "    rf = RD(prod_data)\n",
    "    add_feature(ReviewFeatures, rf, ['RD'])\n",
    "\n",
    "    rf = EXT(prod_data)\n",
    "    add_feature(ReviewFeatures, rf, ['EXT'])\n",
    "\n",
    "    rf = DEV(prod_data)\n",
    "    add_feature(ReviewFeatures, rf, ['DEV'])\n",
    "\n",
    "\t#rf = ETF(prod_data)\n",
    "\t#add_feature(ReviewFeatures, rf, ['ETF'])\n",
    "\n",
    "    rf = ISR(prod_data)\n",
    "    add_feature(ReviewFeatures, rf, ['ISR'])\n",
    "\n",
    "# add low level text features\n",
    "\t##print ('\\nadding low level review features......\\n')\n",
    "\t##for k, v in text_features.items():\n",
    "\t##\t# k is a tuple (u_id, p_id)\n",
    "\t##\t# v is a dict (key=feature name, value = feature value)\n",
    "\t##\tnames = [name for name, value in v.items()]\n",
    "\t##\tvalues = [value for name, value in v.items()]\n",
    "\n",
    "# recall that the arguments to add_feature\n",
    "\t##\tadd_feature(ReviewFeatures, {k:values}, names)\n",
    "\n",
    "    return UserFeatures, ProdFeatures, ReviewFeatures\n",
    "\n",
    "def calculateNodePriors(node_features, when_suspicious):\n",
    "\t\"\"\"\n",
    "\t\tCalculate priors of nodes P(y=1|node) using node features.\n",
    "\t\tArgs:\n",
    "\t\t\tnode_features: a dictionary with key = node_id and value = dict of feature_name:feature_value\n",
    "\t\t\twhen_suspicious: a dictionary with key = feature name and value = 'H' (the higher the more suspicious) or 'L' (the opposite)\n",
    "\t\tReturn:\n",
    "\t\t\tA dictionary with key = node_id and value = S score (see the SpEagle paper for the definition)\n",
    "\t\"\"\"\n",
    "# return value\n",
    "\tpriors = {}\n",
    "\n",
    "\tfeature_names = set()\n",
    "\tfor k, v in node_features.items():\n",
    "\t\tfor fn, fv in v.items():\n",
    "\t\t\t# only include those in when_suspicious\n",
    "\t\t\tif fn in when_suspicious:\n",
    "\t\t\t\tfeature_names.add(fn)\n",
    "\n",
    "\tfeature_names = list(feature_names)\n",
    "\tn_features = len(feature_names)\n",
    "\t\n",
    "\tprint ('number of features: %d' % n_features)\n",
    "\n",
    "\tfeature_values = {name:[] for name in feature_names}\n",
    "\n",
    "\t# go through the instances\n",
    "\tfor k, v in node_features.items():\n",
    "\t\t# go through the features\n",
    "\t\tfor fn in feature_names:\n",
    "\t\t\tif fn not in v:\n",
    "\t\t\t\tfeature_values[fn].append(0)\t\t# add 0 by defaulty\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeature_values[fn].append(v[fn])\t# add the non-zero feature values\n",
    "\n",
    "\t\n",
    "\tfor k,v in feature_values.items():\n",
    "\t\tassert len(v) == len(node_features), 'number of feature values is different from number of instances'\n",
    "\n",
    "\t#def ecdf(x):\n",
    "\t#    x = np.sort(x)\n",
    "\t#\tdef result(v):\n",
    "\t#\t\t\t        return np.searchsorted(x, v, side='right') / x.size\n",
    "\t#\t\t\t\t    return result\n",
    "\t\n",
    "\t# compute CDF for each feature\n",
    "\tall_cdfs = {}\n",
    "\tfor fn in feature_names:\n",
    "\t\tprint (fn)\n",
    "\t\tif fn not in feature_values:\n",
    "\t\t\tprint (fn + ' not in feature values')\n",
    "\t\t\tcontinue\n",
    "\t\tbins = sorted(list(set(feature_values[fn])))\n",
    "\t\t#if fn == 'DEV':\n",
    "\t\tprint (bins[0])\n",
    "\t\tif len(bins) == 1:\n",
    "\t\t\tprint (fn + ' has only one value.')\n",
    "# bin1=[a,b), bin2=[b,c), ... last_bin = [y,z]\n",
    "\t\th, d = np.histogram(feature_values[fn], bins = bins)\n",
    "\t\tcdf = np.cumsum(h)\n",
    "\t\tnormalizer= 1.0 / cdf[-1]\n",
    "\t\tcdf = normalizer * cdf\n",
    "\n",
    "# for later lookup\n",
    "\t\tprint (len(bins), len(d), len(cdf))\n",
    "\t\tall_cdfs[fn] = {d[i]:cdf[i] for i in range(1, len(d)-1)}\n",
    "\t\tall_cdfs[fn][d[0]] = 0\t# the least value has cdf 0\n",
    "\t\tall_cdfs[fn][d[-1]] = 1\t# the least value has cdf 0\n",
    "\n",
    "\t#print (list(all_cdfs['BST'].keys()))\n",
    "# for each node, find the S score for each feature\n",
    "\tfor k, v in node_features.items():\n",
    "\t\ts = 0\n",
    "\t\tfor fn in feature_names:\n",
    "\t\t\tx = 0 if fn not in v else v[fn]\n",
    "\t\t\tc = all_cdfs[fn][x]\n",
    "\t\t\tfx = 1.0 - c if when_suspicious[fn] == '+' else c\n",
    "\t\t\ts += fx * fx\n",
    "\t\ts = 1 - np.sqrt(s / n_features)\n",
    "\t\tpriors[k] = s\n",
    "\t\n",
    "\treturn priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Priors(metadata_filename, u_data, p_data):\n",
    "\n",
    "# path to the folder containing the files\n",
    "\t#prefix = '/home/sihong/data/yelp_reviews/'\n",
    "\t#prefix = '/Users/bon/Documents/data mining/datasets/yelp_reviews/nyc/'\n",
    "    prefix = '/Users/anahita/datasets/YelpNYC/'\n",
    "\n",
    "# raw data file names\n",
    "    #metadata_filename = prefix + 'metadata'\n",
    "    #review_filename = prefix + 'reviewContent'\n",
    "\n",
    "# feature file names\n",
    "    user_feature_filename = prefix + 'UserFeatures.pickle'\n",
    "    prod_feature_filename = prefix + 'ProdFeatures.pickle'\n",
    "    review_feature_filename = prefix + 'ReviewFeatures.pickle'\n",
    "\n",
    "# prior file names\n",
    "    user_prior_filename = prefix + 'UserPriors.pickle'\n",
    "    prod_prior_filename = prefix + 'ProdPriors.pickle'\n",
    "    review_prior_filename = prefix + 'ReviewPriors.pickle'\n",
    "\n",
    "# output file names\n",
    "    text_feature_filename = prefix + 'text_features.pickle'\n",
    "\n",
    "# feature configuration\n",
    "    feature_suspicious_filename = 'feature_configuration.txt'\n",
    "\n",
    "# low level text features\n",
    "\t# just need to construct text features once\n",
    "\t#print ('Starting constructing low level text features\\n')\n",
    "\t#low_level_text_features(review_filename, text_feature_filename)\n",
    "\n",
    "\t# can just load the constructed low level text features\n",
    "\t#tf = load_text_feature(text_feature_filename)\n",
    "\t#print ('Finished constructing low level text features\\n')\n",
    "\n",
    "# all high level features\n",
    "    print ('Starting constructing high level user, product and review features\\n')\n",
    "    user_data = {}\n",
    "    prod_data = {}\n",
    "    if(metadata_filename != ' '):\n",
    "        user_data, prod_data = read_graph_data(metadata_filename)\n",
    "    else:\n",
    "        user_data = u_data\n",
    "        prod_data = p_data\n",
    "        \n",
    "    text_features = []\n",
    "    #text_features = load_text_feature(text_feature_filename)\n",
    "    UserFeatures, ProdFeatures, ReviewFeatures = construct_all_features(user_data, prod_data, text_features)\n",
    "    with open(user_feature_filename, 'wb') as f:\n",
    "        pickle.dump(UserFeatures, f)\n",
    "\n",
    "    with open(prod_feature_filename, 'wb') as f:\n",
    "        pickle.dump(ProdFeatures, f)\n",
    "\n",
    "    with open(review_feature_filename, 'wb') as f:\n",
    "        pickle.dump(ReviewFeatures, f)\n",
    "    print ('Finished constructing high level user, product and review features\\n')\n",
    "\n",
    "# Priors\n",
    "    print ('Start calculating user, product and review priors.\\n')\n",
    "\n",
    "    feature_config = load_feature_config(feature_suspicious_filename)\n",
    "    print (feature_config)\n",
    "\n",
    "    with open(user_feature_filename, 'rb') as f:\n",
    "        user_features = pickle.load(f)\n",
    "    user_priors = calculateNodePriors(user_features, feature_config)\n",
    "    with open(user_prior_filename, 'wb') as f:\n",
    "        pickle.dump(user_priors, f)\n",
    "\n",
    "    with open(prod_feature_filename, 'rb') as f:\n",
    "        prod_features = pickle.load(f)\n",
    "    prod_priors = calculateNodePriors(prod_features, feature_config)\n",
    "    with open(prod_prior_filename, 'wb') as f:\n",
    "        pickle.dump(prod_priors, f)\n",
    "\n",
    "    #with open(review_feature_filename, 'rb') as f:\n",
    "    #    review_features = pickle.load(f)\n",
    "    #review_priors = calculateNodePriors(review_features, feature_config)\n",
    "    #with open(review_prior_filename, 'wb') as f:\n",
    "    #    pickle.dump(review_priors, f)\n",
    "    print ('Finished calculating user, product and review priors.\\n')\n",
    "    \n",
    "    return (user_priors, prod_priors, user_data, prod_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    implementation of ZooBP with its helper functions\n",
    "'''\n",
    "def Initialize_Final_Beliefs(N1, N2, m):\n",
    "    '''\n",
    "        Initialization of final beliefs\n",
    "        Args:\n",
    "            N1: number of users\n",
    "            N2: number of products\n",
    "            m: coefficient for reduction in beliefs\n",
    "        Returns:\n",
    "            Concatenation of initialized final beliefs for users and products\n",
    "        Example of return values: -0.5 0.5 -0.3 0.3 ...\n",
    "    '''\n",
    "    r1 = m * (np.random.uniform(size=N1) - 0.5)\n",
    "    r1 = r1.reshape(r1.shape[0], 1)\n",
    "    r2 = m * (np.random.uniform(size=N2) - 0.5)\n",
    "    r2 = r2.reshape(r2.shape[0], 1)\n",
    "    B1 = np.concatenate((r1, -r1), axis = 1);\n",
    "    B2 = np.concatenate((r2, -r2), axis = 1);\n",
    "    \n",
    "    temp1_B = B1.reshape((B1.shape[1]*B1.shape[0], 1))\n",
    "    temp2_B = B2.reshape((B2.shape[1]*B2.shape[0], 1))\n",
    "    B = np.concatenate((temp1_B, temp2_B), axis = 0)\n",
    "   \n",
    "    return B\n",
    "\n",
    "\n",
    "def ZooBPPlus(a_list, u_priors, p_priors, fake_indices, nonfake_indices, ep, H):\n",
    "    '''\n",
    "        implementation of ZooBP in python\n",
    "        Args:\n",
    "            a_list: [user_id, prod_id, rating]\n",
    "            u_priors: user priors in 2 classes: benign, spammer\n",
    "            p_priors: prod priors in 2 classes: non-targeted, targeted\n",
    "            fake_indices: indices of labeled spammers\n",
    "            nonfake_indices: indices of labeled nonspammers\n",
    "            ep: interaction strenght\n",
    "            H: compatibility matrix\n",
    "        Returns:\n",
    "            final_user_beliefs: centered version of final user beliefs\n",
    "            final_prod_beliefs ceneterd version of final prod beliefs\n",
    "        NOTE:\n",
    "            ZooBP requires consecutive ids not ids with gaps\n",
    "    '''\n",
    "    ##adjlist = AdjacencyList(metadata_filename)\n",
    "    \n",
    "    ##convert a_list to adjlist [user_id, prod_id, 1/2], where 1 indicates positive rating (4, 5) \n",
    "    ##and 2 indicates negative rating (1, 2, 3)\n",
    "    adjlist = np.zeros((a_list.shape[0], 3))\n",
    "    unq_user, user_tags = np.unique(a_list[:,0],return_inverse=1)\n",
    "    unq_prod, prod_tags = np.unique(a_list[:,1],return_inverse=1)\n",
    "    adjlist[:, 0] = user_tags\n",
    "    adjlist[:, 1] = prod_tags\n",
    "\n",
    "    adjlist[:, 0] = user_tags + np.ones(user_tags.shape[0])\n",
    "    adjlist[:, 1] = prod_tags + np.ones(prod_tags.shape[0])\n",
    "    adjlist[:, 2] = a_list[:, 2]\n",
    "    \n",
    "    \n",
    "    rating = adjlist[:, 2]\n",
    "    adjlist[rating == 1, 2] = -1\n",
    "    adjlist[rating == 2, 2] = -1\n",
    "    adjlist[rating == 3, 2] = -1\n",
    "    adjlist[adjlist[:, 2] == -1, 2] = 2\n",
    "    adjlist[rating == 4, 2] = -2\n",
    "    adjlist[rating == 5, 2] = -2\n",
    "    adjlist[adjlist[:, 2] == -2, 2] = 1\n",
    "    rating = adjlist[:, 2]\n",
    "    #print(adjlist[0:5, :])\n",
    "    ##converts the given priors to the centered version \n",
    "    user_priors = u_priors - 0.5 * np.ones((u_priors.shape[0], 2))\n",
    "    prod_priors = p_priors - 0.5 * np.ones((p_priors.shape[0], 2))\n",
    "    \n",
    "    ##finds positive (1) and negative (2) edges and reshapes them\n",
    "    edges_pos = adjlist[rating == 1]\n",
    "    edges_neg = adjlist[rating == 2]\n",
    "    Lpos = edges_pos[:, 0:2]\n",
    "    Lpos = Lpos.reshape((edges_pos.shape[0], 2))\n",
    "    Lneg = edges_neg[:, 0:2]\n",
    "    Lneg = Lneg.reshape((edges_neg.shape[0], 2))\n",
    "    \n",
    "    n_user = user_priors.shape[0]\n",
    "    n_prod = prod_priors.shape[0]\n",
    "    \n",
    "    ##computes A+ and A- as defined in section 4.7 of ZooBP\n",
    "    lpos_0 = Lpos[:,0] - np.ones(Lpos[:,0].shape[0])\n",
    "    lpos_1 = Lpos[:,1] - np.ones(Lpos[:,1].shape[0])\n",
    "    Apos = sparse.coo_matrix((np.ones(Lpos.shape[0]), (lpos_0, lpos_1)), shape=(n_user, n_prod))\n",
    "    lneg_0 = Lneg[:,0] - np.ones(Lneg[:,0].shape[0])\n",
    "    lneg_1 = Lneg[:,1] - np.ones(Lneg[:,1].shape[0])\n",
    "    Aneg = sparse.coo_matrix((np.ones(len(Lneg)), (lneg_0, lneg_1)), shape=(n_user, n_prod))\n",
    "\n",
    "    \n",
    "    #prior beliefs are reshaped so that user1_belief 1-user1_belief ... prod1_belief 1-prod1_belief\n",
    "    reshape_u = user_priors.reshape((2*n_user, 1))\n",
    "    reshape_p = prod_priors.reshape((2*n_prod, 1))\n",
    "    E = np.concatenate((reshape_u, reshape_p))\n",
    "    \n",
    "    #build P defined under section 4.7 of ZooBP\n",
    "    R = sparse.kron(Apos-Aneg, ep*H)\n",
    "    sp1 = sparse.identity(2*n_user)-sparse.identity(2*n_user)\n",
    "    temp1 = sparse.hstack([sp1, 0.5 * R])\n",
    "    sp2 = sparse.identity(2*n_prod)-sparse.identity(2*n_prod)\n",
    "    temp2 = sparse.hstack([0.5*R.transpose(),sp2])\n",
    "    P = sparse.vstack((temp1, temp2))\n",
    "    P = P.transpose()\n",
    "\n",
    "    \n",
    "    \n",
    "    #build Q defined under section 4.7 of ZooBP\n",
    "    sum_temp = Apos + Aneg\n",
    "    temp1 = sum_temp.sum(axis=1)\n",
    "    temp2 = sum_temp.sum(axis=0)\n",
    "    ##D12 = sparse.diags(temp1,0,(temp1.shape[0],temp1.shape[0]))\n",
    "    ##D12 = sparse.csr_matrix(np.diagflat(temp1))\n",
    "    temp11 = np.matrix(temp1)\n",
    "    temp111 = temp11.tolist()\n",
    "    flat_temp1 = [item for sublist in temp111 for item in sublist]\n",
    "    D12 = sparse.diags(flat_temp1, 0)\n",
    "    \n",
    "    temp22 = np.matrix(temp2)\n",
    "    temp222 = temp22.tolist()\n",
    "    flat_temp2 = [item for sublist in temp222 for item in sublist]\n",
    "    D21 = sparse.diags(flat_temp2, 0)\n",
    "    #D21 = sparse.csr_matrix(np.diagflat(temp2))\n",
    "    del temp11\n",
    "    del temp111\n",
    "    del temp22\n",
    "    del temp222\n",
    "    \n",
    "    \n",
    "    temp = 0.25 * ep * ep * sparse.kron(D12, H)\n",
    "    Q_1 = sparse.eye(n_user * 2) + temp      \n",
    "    Q_2 = sparse.eye(n_prod * 2) + (0.25 * ep * ep) * (sparse.kron(D21, H))\n",
    "    sp1 = sparse.csr_matrix(np.zeros((n_user * 2, n_prod * 2)))\n",
    "    Q_temp1 = sparse.hstack((Q_1, sp1))\n",
    "    sp2 = sparse.csr_matrix(np.zeros((n_prod * 2, n_user * 2)))\n",
    "    Q_temp2 = sparse.hstack((sp2, Q_2))\n",
    "    Q = sparse.vstack((Q_temp1, Q_temp2))\n",
    "    del temp\n",
    "    del Q_1\n",
    "    del Q_2\n",
    "    del Q_temp1\n",
    "    del Q_temp2\n",
    "    \n",
    "    #M\n",
    "    \n",
    "    M = P - Q + sparse.eye(2 * (n_user + n_prod))\n",
    "    M = M.transpose()\n",
    "    B = Initialize_Final_Beliefs(n_user, n_prod, 0.0001)\n",
    "    \n",
    "    #Iterative Solution\n",
    "    res = 1\n",
    "    while(res > 1e-8):\n",
    "        Bold = B\n",
    "        ##Equations (13) and (14) in ZooBP\n",
    "        B = E + M * Bold\n",
    "        \n",
    "        #blocked users for fake indices\n",
    "        for index in fake_indices:\n",
    "            B[2*index] = 0.999 - 0.5\n",
    "            B[2*index + 1] = 0.001 - 0.5\n",
    "            \n",
    "        #blocked users for nonfake indices\n",
    "        for index in nonfake_indices:\n",
    "            B[2*index] = 0.001 - 0.5\n",
    "            B[2*index + 1] = 0.999 - 0.5         \n",
    "            \n",
    "        res = np.sum(np.sum(abs(Bold-B)))\n",
    "    \n",
    "    B1 = B[0:2*n_user,:]\n",
    "    B2 = B[2*n_user:,:]\n",
    "    user_beliefs= B1.reshape((n_user, 2))\n",
    "    prod_beliefs= B2.reshape((n_prod, 2))\n",
    "    \n",
    "    return (user_beliefs, prod_beliefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreZooBP(metadata_filename):\n",
    "    \"\"\"\n",
    "       \n",
    "        Args:\n",
    "            metadata_filename: adjacency list [user, product, rating, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    #adjacency list\n",
    "    adjlist = AdjacencyList(metadata_filename)\n",
    "    \n",
    "    #user and product priors\n",
    "    #pickle_user = open(user_priors_filename,\"rb\")\n",
    "    #user_dict = pickle.load(pickle_user)\n",
    "    #user_priors = np.zeros((6, 3))\n",
    "    #user_priors[1, 0] = 1\n",
    "    \n",
    "    prefix = '/Users/anahita/datasets/YelpNYC/'\n",
    "    \n",
    "    print('Reshaping user and prod priors \\n')\n",
    "    Results = Priors(metadata_filename, [], [])\n",
    "    u_priors = Results[0]\n",
    "    user_priors = ReshapePriors(u_priors)\n",
    "    \n",
    "    p_priors = Results[1]\n",
    "    prod_priors = ReshapePriors(p_priors)\n",
    "    print('Done reshaping\\n')\n",
    "    \n",
    "    user_data = Results[2]\n",
    "    prod_data = Results[3]\n",
    "    \n",
    "    with open(prefix + 'userPriors.csv', 'w') as user_csvfile:\n",
    "        writer = csv.writer(user_csvfile)\n",
    "        for i in range(0, user_priors.shape[0]):\n",
    "            writer.writerow(user_priors[i, :])\n",
    "\n",
    "    with open(prefix + 'prodPriors.csv', 'w') as prod_csvfile:\n",
    "        writer = csv.writer(prod_csvfile)\n",
    "        for i in range(0, prod_priors.shape[0]):\n",
    "            writer.writerow(prod_priors[i, :])\n",
    "            \n",
    "    with open(prefix + 'adjlist.csv', 'w') as adjlist_csvfile:\n",
    "        writer = csv.writer(adjlist_csvfile)\n",
    "        for i in range(0, adjlist.shape[0]):\n",
    "            writer.writerow(adjlist[i, :])\n",
    "            \n",
    "    print('Done saving\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def MainZooBPSemiSupervised(adjlist, labeled_spammers, labeled_nonspammers, ep, H):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            ep: interaction strength\n",
    "            H: compatibility matrix\n",
    "        Return:\n",
    "            added edges\n",
    "        Assumptions: \n",
    "            only one bad targeted product\n",
    "    \"\"\"\n",
    "    #prefix = '/Users/anahita/datasets/YelpNYC/'\n",
    "    #prefix = '/home/mnajaf2/ZooBP'\n",
    "    #prefix = '/Users/anahita/Documents/U/U2016/UIC/Research/OpinionSpamDetection/Code/src/graph/ZooBP/'\n",
    "    prefix = '/Users/anahita/Documents/U/U2016/UIC/Research/OpinionSpamDetection/Code/src/graph/ZooBP/ZooBPPySemiSupervisedPy/'\n",
    "        \n",
    "    ###user_priors = np.zeros((6, 3))\n",
    "    ###prod_priors = np.zeros((4, 3))\n",
    "    ###adjlist = np.zeros((14, 3))\n",
    "    user_priors = np.zeros((160225, 3))\n",
    "    prod_priors = np.zeros((923, 3))\n",
    "    #adjlist = np.zeros((359052, 3))\n",
    "    \n",
    "    #split_per = 0.5\n",
    "    \n",
    "    #resultsA = AdjacencyList('metadata')\n",
    "    #adjlist = resultsA[0]\n",
    "    \n",
    "    #what about prefix?\n",
    "    ##i = 0\n",
    "    ##with open(prefix + 'adjlist.csv', 'r') as adjlist_csvfile:\n",
    "    ##    reader = csv.reader(adjlist_csvfile)\n",
    "    ##    for row in reader:\n",
    "    ##        adjlist[i, 0] = float(row[0])\n",
    "    ##        adjlist[i, 1] = float(row[1])\n",
    "    ##        adjlist[i, 2] = float(row[2])\n",
    "    ##        i = i + 1\n",
    "    #print(adjlist[0:10, :])\n",
    "    \n",
    "    \n",
    "    ##results = Random_Split(split_per, resultsA[1], resultsA[2])\n",
    "    ##labeled_spammers = results[0]\n",
    "    ##labeled_nonspammers = results[1]\n",
    "    \n",
    "    i = 0\n",
    "    with open(prefix + 'userPriors.csv', 'r') as user_csvfile:\n",
    "        reader = csv.reader(user_csvfile)\n",
    "        for row in reader:\n",
    "            user_priors[i, 0] = float(row[0])\n",
    "            user_priors[i, 1] = float(row[1])\n",
    "            user_priors[i, 2] = float(row[2])\n",
    "            i = i + 1\n",
    "            \n",
    "    #print(user_priors[0:10, :])\n",
    "    u_priors = user_priors[np.argsort(user_priors[:, 0])]\n",
    "    user_priors = u_priors\n",
    "    \n",
    "    #assign values for labeled data\n",
    "    fake_indices = np.zeros(labeled_spammers.shape[0])\n",
    "    for i in range(0, labeled_spammers.shape[0]):\n",
    "        idx = np.where(user_priors[:, 0] == labeled_spammers[i])\n",
    "        fake_indices[i] = idx[0]\n",
    "    \n",
    "    \n",
    "    #1:spammer 2:benign\n",
    "    for i in range(0, labeled_spammers.shape[0]):\n",
    "        user_priors[fake_indices[i], 1] = .999\n",
    "        user_priors[fake_indices[i], 2] = .001\n",
    "    \n",
    "    nonfake_indices = np.zeros(labeled_nonspammers.shape[0])\n",
    "    for i in range(0, labeled_nonspammers.shape[0]):\n",
    "        idx = np.where(user_priors[:, 0] == labeled_nonspammers[i])\n",
    "        nonfake_indices[i] = idx[0]\n",
    "    \n",
    "    #1:spammer 2:benign\n",
    "    for i in range(0, labeled_nonspammers.shape[0]):\n",
    "        user_priors[nonfake_indices[i], 1] = .001\n",
    "        user_priors[nonfake_indices[i], 2] = .999\n",
    "        \n",
    "    \n",
    "    i = 0\n",
    "    with open(prefix + 'prodPriors.csv', 'r') as prod_csvfile:\n",
    "        reader = csv.reader(prod_csvfile)\n",
    "        for row in reader:\n",
    "            prod_priors[i, 0] = float(row[0])\n",
    "            prod_priors[i, 1] = float(row[1])\n",
    "            prod_priors[i, 2] = float(row[2])\n",
    "            i = i + 1\n",
    "    #print(prod_priors[0:10, :])\n",
    "    p_priors = prod_priors[np.argsort(prod_priors[:, 0])]\n",
    "    prod_priors = p_priors    \n",
    "    \n",
    "    #user_priors, prod_priors, ep, H\n",
    "    print('Starting running SemiSupervised ZooBP\\n')\n",
    "    Results = ZooBPPlus(adjlist, user_priors[:, 1:], prod_priors[:, 1:], \n",
    "                        fake_indices, nonfake_indices, ep, H)\n",
    "    user_beliefs = Results[0]\n",
    "    print('Done running SemiSupervised ZooBP\\n')\n",
    "    \n",
    "    #writing beliefs on the file\n",
    "    final_beliefs = np.zeros((user_beliefs.shape[0], 3))\n",
    "    final_beliefs[:, 0] = user_priors[:, 0]\n",
    "    final_beliefs[:, 1:] = user_beliefs\n",
    "    with open('ZooBPSemiSupervised.csv', 'w') as user_csvfile:\n",
    "        writer = csv.writer(user_csvfile)\n",
    "        for i in range(0, user_beliefs.shape[0]):\n",
    "            writer.writerow(final_beliefs[i, :])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
